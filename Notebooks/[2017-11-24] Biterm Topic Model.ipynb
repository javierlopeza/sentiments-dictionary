{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biterm Topic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biterm Topic Model (BTM) es un modelo de tópicos basado en la co-ocurrencia de palabras que aprende los tópicos al modelar patrones de co-ocurrencia palabra-palabra (es decir, biterms). En constraste, LDA y PLSA son modelos de tópicos basados en co-ocurrencias de palabra-documento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un _biterm_ consiste en dos palabras co-ocurriendo en el mismo contexto, por ejemplo, en la misma ventana de texto. En contraste con los modelos LDA, BTM modela las ocurrencias de biterms en un corpus. En el proceso de generación, un biterm es generado al extraer dos pañabras independientes del mismo tópico. En otras palabras, la distribución de un biterm $b=(wi, wj)$ se define como:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(b) = \\sum_k{P(w_i \\:\\vert\\: z) \\cdot P(w_j \\:\\vert\\: z) \\cdot P(z)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el algoritmo de muestro de Gibbs, podemos extraer los tópicos al estimar $P(w \\:\\vert\\: k)$ y $P(z)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación de BTM (prueba inicial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación procederemos a ejecutar el código de una implementación de BTM desarrollada por [xiaohuiyan](https://github.com/xiaohuiyan/). El código fuente se puede encontrar en su [repositorio](https://github.com/xiaohuiyan/BTM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La estructura del proyecto es la siguiente:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "BTM\n",
    "│   README.md   \n",
    "│\n",
    "└───output\n",
    "│   │\n",
    "│   └───model\n",
    "│\n",
    "└───sample-data\n",
    "│   │   doc_info.txt\n",
    "│   \n",
    "└───script\n",
    "│   │   indexDocs.py\n",
    "│   │   runExample.sh\n",
    "│   │   topicDisplay.py\n",
    "│\n",
    "└───src\n",
    "    │   ...core de la implementación en C...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para probar esta implementación usaremos la data de ejemplo entregada en el directorio `sample-data`. De ahora en adelante se hablará de documentos correpondiendo a cada línea del archivo `doc_info.txt`. Se puede notar que cada documento tiene una extensión pequeña (como un tweet)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer paso es **indexar las palabras en los documentos**. Esto es, mapear cada palabra de los documentos a un ID único comenzando desde el 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index file: ./BTM/sample-data/doc_info.txt\n",
      "write file: ./BTM/output/doc_wids.txt\n",
      "n(w)=28634\n",
      "write:./BTM/output/voca.txt\n"
     ]
    }
   ],
   "source": [
    "%run ./BTM/script/indexDocs.py ./BTM/sample-data/doc_info.txt ./BTM/output/doc_wids.txt ./BTM/output/voca.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posterior a la ejecución de este script, obtendremos dos archivos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`doc_wids.txt`: contiene los documentos traducidos con cada palabra reemplazada por el ID asignado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "0 1 2 3 4\n",
    "5 6\n",
    "7 8 9 10 11 12 9 7 8 9 10 11 13\n",
    "14 15 16 17 17\n",
    "18 19 20\n",
    "21 22 23 24\n",
    "25 26 27 28\n",
    "29 30 31\n",
    "32 33 34 35 36 37 38 39 40 41\n",
    "42 43 44 45\n",
    "46 47 48 49\n",
    "50 51\n",
    "52 53\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`voca.txt`: contiene el vocabulario generado posterior al indexado. En otras palabras, contiene los pares `ID  palabra`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "0\tipod\n",
    "1\ttumblr\n",
    "2\tapp\n",
    "3\tworking\n",
    "4\tfine\n",
    "5\tyup\n",
    "6\tlonger\n",
    "7\tjungle\n",
    "8\tabduction\n",
    "9\talive\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El segundo paso corresponde a **topic learning**. Este paso corresponde a entrenar el modelo usando los documentos representados por los IDs de las palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Nothing to be done for `all'.\n",
      "Run BTM, K=20, W=28634, alpha=2.5, beta=0.005, n_iter=5, save_step=501 ====\n",
      "load docs: ./BTM/output/doc_wids.txt\n",
      "Begin iteration\n",
      "\r",
      "iter 1/5\r",
      "iter 2/5\r",
      "iter 3/5\r",
      "iter 4/5\r",
      "iter 5/5\n",
      "write p(z): ./BTM/output/model/k20.pz\n",
      "write p(w|z): ./BTM/output/model/k20.pw_z\n",
      "cost 3.000405s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# number of topics\n",
    "K=20\n",
    "# alpha, beta\n",
    "alpha=`echo \"scale=3;50/$K\"|bc`\n",
    "beta=0.005\n",
    "# number of iterations\n",
    "niter=5\n",
    "save_step=501\n",
    "# vocabulary size\n",
    "W=`wc -l < ./BTM/output/voca.txt`\n",
    "make -C ./BTM/src\n",
    "./BTM/src/btm est $K $W $alpha $beta $niter $save_step ./BTM/output/doc_wids.txt ./BTM/output/model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados serán escritos en el directorio `output/model/`:\n",
    "\n",
    "`k20.pw_z`: una matriz de K*M para P(w|z), con K=20\n",
    "\n",
    "`k20.pz`: una matriz de K*1 para P(z), con K=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El tercer paso es **inferir las proporciones de tópicos para cada uno de los documentos**, es decir, P(z|d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run inference:K=20, type sum_b ====\n",
      "load p(z):./BTM/output/model/k20.pz\n",
      "load p(w|z):./BTM/output/model/k20.pw_z\n",
      "n(z)=20, n(w)=28634\n",
      "Infer p(z|d) for docs in: ./BTM/output/doc_wids.txt\n",
      "write p(z|d): ./BTM/output/model/k20.pz_d\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "K=20\n",
    "./BTM/src/btm inf sum_b $K ./BTM/output/doc_wids.txt ./BTM/output/model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado será escrito en el directorio `output/model/`:\n",
    "\n",
    "`k20.pz_d`: una matriz de N*K para P(z|d), con K=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El cuarto, y último paso, será **presentar las palabras más frecuentes por cada tópico y sus proporciones en la colección**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K:20, n(W):28634\n",
      "p(z)\t\tTop words\n",
      "0.056884\t0:0.017529 c:0.006421 wind:0.006228 rain:0.004687 00:0.004259 sunday:0.004045 humidity:0.003382 30:0.003339 km:0.003211 12:0.003146\n",
      "0.053463\tlil:0.003370 social:0.002801 makes:0.002573 11:0.002573 fair:0.002482 nigga:0.002436 girls:0.002368 party:0.002345 l:0.002322 years:0.002300\n",
      "0.052334\tcuz:0.003628 boxes:0.003372 wit:0.002954 online:0.002954 bedroom:0.002512 person:0.002489 dnt:0.002442 em:0.002419 bitch:0.002396 hair:0.002349\n",
      "0.052332\t0:0.004326 left:0.003908 c:0.003815 state:0.003210 20:0.003187 22:0.002884 30:0.002745 run:0.002652 pm:0.002559 basketball:0.002349\n",
      "0.051819\tparty:0.004792 apple:0.003194 store:0.003171 online:0.002866 iphone:0.002819 app:0.002584 ready:0.002537 hit:0.002396 head:0.002067 concert:0.002067\n",
      "0.051702\taward:0.005014 nominate:0.003296 shorty:0.003272 eat:0.002943 360:0.002754 food:0.002590 hours:0.002566 photo:0.002519 loss:0.002472 dinner:0.002425\n",
      "0.051340\tjustin:0.004931 hair:0.003580 die:0.003532 ik:0.003058 een:0.003034 dat:0.002869 room:0.002845 bieber:0.002821 friend:0.002750 read:0.002726\n",
      "0.051309\tadam:0.004388 unfbert:0.003985 lady:0.003558 em:0.003202 ready:0.002965 girls:0.002775 aint:0.002443 cuz:0.002325 brothers:0.002159 na:0.002088\n",
      "0.050427\tcrazy:0.003041 bieber:0.003041 awesome:0.002944 part:0.002534 high:0.002486 makes:0.002438 room:0.002414 favorite:0.002365 face:0.002293 movie:0.002100\n",
      "0.049606\teu:0.005005 por:0.004759 para:0.004489 caga:0.004440 te:0.003901 q:0.003852 pra:0.003778 se:0.003631 mi:0.003533 la:0.003435\n",
      "0.049484\teu:0.011583 tmb:0.005730 pra:0.004943 um:0.004919 aqui:0.004230 na:0.004181 minha:0.003837 q:0.003443 mas:0.003394 la:0.003369\n",
      "0.048832\tf:0.004461 ha:0.003464 job:0.002916 mph:0.002841 winds:0.002691 en:0.002542 0:0.002467 fair:0.002118 movie:0.002093 9:0.002019\n",
      "0.048554\tdat:0.003885 die:0.002957 head:0.002782 nah:0.002607 k:0.002406 wat:0.002406 fast:0.002356 card:0.002005 mom:0.001955 shot:0.001880\n",
      "0.048163\t9:0.009045 ft:0.006518 faster:0.004396 wave:0.003689 jets:0.003436 coming:0.002754 city:0.002653 j:0.002628 post:0.002426 worth:0.002375\n",
      "0.048148\thit:0.007683 years:0.005636 fight:0.003690 website:0.003134 ago:0.002881 20:0.002856 lost:0.002730 app:0.002628 amazing:0.002628 heath:0.002401\n",
      "0.047616\tla:0.006056 lo:0.003654 en:0.002785 full:0.002581 hay:0.002453 fit:0.002402 si:0.002402 birthday:0.002351 makes:0.002300 mi:0.002300\n",
      "0.047572\twomen:0.003325 amp:0.002916 hot:0.002353 part:0.002225 mom:0.002149 business:0.002097 sex:0.002021 50:0.001944 followers:0.001944 men:0.001867\n",
      "0.047404\tha:0.004158 reading:0.002772 car:0.002593 em:0.002259 head:0.002233 knock:0.002156 read:0.002131 eyes:0.002105 times:0.002079 years:0.002079\n",
      "0.046854\t0:0.004051 wit:0.003064 c:0.002986 12:0.002545 top:0.002467 di:0.002467 jets:0.002441 album:0.002285 found:0.002181 22:0.002129\n",
      "0.046156\tfeature:0.007750 channel:0.006010 autoshare:0.005298 called:0.005272 00:0.004165 subscribed:0.003954 01:0.003427 dude:0.003374 justin:0.003242 sunday:0.002952\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "K=20\n",
    "python3 ./BTM/script/topicDisplay.py ./BTM/output/model/ $K ./BTM/output/voca.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos como varios de los tópicos presentados presentan un set de palabras adecuadas que permiten inferir de manera directa el tópico encontrado. Otros tópicos no son muy precisos, por lo que no se puede inferir un tópico claro del set asociado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usando BTM con dataset de tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, procederemos a ejecutar la misma secuencia de scripts ahora para los datasets de tweets parseados en los notebooks anteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Indexar las palabras en los documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== Index Docs =============\n",
      "index file: ./BTM/sample-data/tweets-train.txt\n",
      "write file: ./BTM/output-tweets/doc_wids.txt\n",
      "n(w)=47857\n",
      "write:./BTM/output-tweets/voca.txt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "input_dir=./BTM/sample-data/\n",
    "output_dir=./BTM/output-tweets/\n",
    "model_dir=${output_dir}model/\n",
    "mkdir -p $output_dir/model \n",
    "\n",
    "# the input docs for training\n",
    "doc_pt=${input_dir}tweets-train.txt\n",
    "\n",
    "echo \"=============== Index Docs =============\"\n",
    "# docs after indexing\n",
    "dwid_pt=${output_dir}doc_wids.txt\n",
    "# vocabulary file\n",
    "voca_pt=${output_dir}voca.txt\n",
    "python3 ./BTM/script/indexDocs.py $doc_pt $dwid_pt $voca_pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Topic Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== Topic Learning =============\n",
      "make: Nothing to be done for `all'.\n",
      "./BTM/src/btm est 20    47857 2.500 0.005 5 501 ./BTM/output-tweets/doc_wids.txt ./BTM/output-tweets/model/\n",
      "Run BTM, K=20, W=47857, alpha=2.5, beta=0.005, n_iter=5, save_step=501 ====\n",
      "load docs: ./BTM/output-tweets/doc_wids.txt\n",
      "Begin iteration\n",
      "\r",
      "iter 1/5\r",
      "iter 2/5\r",
      "iter 3/5\r",
      "iter 4/5\r",
      "iter 5/5\n",
      "write p(z): ./BTM/output-tweets/model/k20.pz\n",
      "write p(w|z): ./BTM/output-tweets/model/k20.pw_z\n",
      "cost 7.670864s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# number of topics\n",
    "K=20\n",
    "\n",
    "alpha=`echo \"scale=3;50/$K\"|bc`\n",
    "beta=0.005\n",
    "niter=5\n",
    "save_step=501\n",
    "\n",
    "input_dir=./BTM/sample-data/\n",
    "output_dir=./BTM/output-tweets/\n",
    "model_dir=${output_dir}model/\n",
    "# the input docs for training\n",
    "doc_pt=${input_dir}tweets-train.txt\n",
    "# docs after indexing\n",
    "dwid_pt=${output_dir}doc_wids.txt\n",
    "# vocabulary file\n",
    "voca_pt=${output_dir}voca.txt\n",
    "\n",
    "## learning parameters p(z) and p(w|z)\n",
    "echo \"=============== Topic Learning =============\"\n",
    "W=`wc -l < $voca_pt` # vocabulary size\n",
    "make -C ./BTM/src\n",
    "echo \"./BTM/src/btm est $K $W $alpha $beta $niter $save_step $dwid_pt $model_dir\"\n",
    "./BTM/src/btm est $K $W $alpha $beta $niter $save_step $dwid_pt $model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inferir las proporciones de tópicos para cada uno de los documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Infer P(z|d)===============\n",
      "./BTM/src/btm inf sum_b 20 ./BTM/output-tweets/doc_wids.txt ./BTM/output-tweets/model/\n",
      "Run inference:K=20, type sum_b ====\n",
      "load p(z):./BTM/output-tweets/model/k20.pz\n",
      "load p(w|z):./BTM/output-tweets/model/k20.pw_z\n",
      "n(z)=20, n(w)=47857\n",
      "Infer p(z|d) for docs in: ./BTM/output-tweets/doc_wids.txt\n",
      "write p(z|d): ./BTM/output-tweets/model/k20.pz_d\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# number of topics\n",
    "K=20\n",
    "\n",
    "output_dir=./BTM/output-tweets/\n",
    "# docs after indexing\n",
    "dwid_pt=${output_dir}doc_wids.txt\n",
    "model_dir=${output_dir}model/\n",
    "\n",
    "## infer p(z|d) for each doc\n",
    "echo \"================ Infer P(z|d)===============\"\n",
    "echo \"./BTM/src/btm inf sum_b $K $dwid_pt $model_dir\"\n",
    "./BTM/src/btm inf sum_b $K $dwid_pt $model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Presentar las palabras más frecuentes por cada tópico y sus proporciones en la colección"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Topic Display =============\n",
      "K:20, n(W):47857\n",
      "p(z)\t\tTop words\n",
      "0.052902\trajoy:0.009470 pp:0.005375 gobierno:0.004940 mañana:0.004247 españa:0.004202 años:0.003882 @telediariointer:0.003740 2030:0.003722 reforma:0.003625 gracias:0.003065\n",
      "0.051914\trajoy:0.009098 psoe:0.005658 @telediariointer:0.004735 españa:0.004409 2030:0.003974 chacon:0.003938 rubalcaba:0.003847 eta:0.003838 gobierno:0.003730 madrid:0.003368\n",
      "0.051597\trajoy:0.007551 gobierno:0.006239 psoe:0.004955 pp:0.004527 congreso:0.003780 mañana:0.003434 gracias:0.003233 rubalcaba:0.003042 madrid:0.002687 reforma:0.002669\n",
      "0.051456\tgobierno:0.007718 rajoy:0.007571 pp:0.004941 españa:0.004521 madrid:0.003854 psoe:0.003681 años:0.003489 millones:0.003169 mañana:0.002886 3:0.002685\n",
      "0.051253\trajoy:0.006474 psoe:0.006446 rubalcaba:0.004722 congreso:0.004649 @mariviromero:0.004521 gracias:0.004475 gobierno:0.004392 mañana:0.004154 pp:0.004108 chacon:0.003108\n",
      "0.050823\trajoy:0.008877 pp:0.006389 gobierno:0.005992 psoe:0.004355 españa:0.003884 ley:0.003810 rubalcaba:0.003551 año:0.003301 mañana:0.003107 congreso:0.002922\n",
      "0.050755\trajoy:0.009740 pp:0.005935 gobierno:0.005713 psoe:0.005055 rubalcaba:0.004620 congreso:0.004555 españa:0.003491 mañana:0.003407 chacon:0.003065 reforma:0.002917\n",
      "0.050359\trajoy:0.007661 gobierno:0.006700 psoe:0.005086 pp:0.004797 rubalcaba:0.004097 mañana:0.003854 año:0.003453 años:0.003350 consejo:0.003182 gracias:0.003070\n",
      "0.050278\trajoy:0.008160 españa:0.006477 psoe:0.004393 gobierno:0.004197 rubalcaba:0.004085 madrid:0.003879 3:0.003440 mañana:0.003028 rey:0.003019 años:0.002982\n",
      "0.050160\trajoy:0.006324 @dham_13tv:0.004422 psoe:0.003972 gracias:0.003729 gobierno:0.003635 pp:0.003429 ja:0.003204 @13tv_aldia:0.003092 mañana:0.002820 congreso:0.002745\n",
      "0.050103\trajoy:0.008507 gobierno:0.004830 mañana:0.004474 3:0.004305 años:0.004024 españa:0.003921 madrid:0.003452 pp:0.003405 millones:0.003339 psoe:0.003142\n",
      "0.049840\tgracias:0.007873 año:0.004856 feliz:0.004620 rajoy:0.004149 @mariviromero:0.003951 2012:0.003706 gobierno:0.003489 mañana:0.003234 |:0.003083 #ff:0.003064\n",
      "0.049703\trajoy:0.008311 psoe:0.005758 gracias:0.005484 gobierno:0.005049 disponible:0.004690 mañana:0.004065 chacon:0.003867 congreso:0.003820 pp:0.003621 historias:0.003442\n",
      "0.049070\trajoy:0.006684 mañana:0.004127 gracias:0.003285 gobierno:0.002969 madrid:0.002863 años:0.002777 garzon:0.002681 noche:0.002653 españa:0.002385 caso:0.002356\n",
      "0.048968\trajoy:0.007696 cont:0.007619 gracias:0.007437 @alejandrosanz:0.007303 gobierno:0.004529 año:0.004376 psoe:0.003944 mañana:0.003483 presidente:0.003263 pp:0.003042\n",
      "0.048921\trajoy:0.005965 gracias:0.004457 gobierno:0.003544 garzon:0.003237 españa:0.003160 pp:0.003007 psoe:0.002910 años:0.002843 millones:0.002728 publico:0.002401\n",
      "0.048619\trajoy:0.007703 gracias:0.006041 gobierno:0.004543 mañana:0.004456 feliz:0.003856 madrid:0.003663 año:0.003557 pp:0.003315 psoe:0.003257 reforma:0.002822\n",
      "0.048531\trajoy:0.007630 gracias:0.005480 gobierno:0.005258 año:0.004773 pp:0.003486 mañana:0.003418 1:0.003263 psoe:0.002905 @mariviromero:0.002740 madrid:0.002508\n",
      "0.048156\trajoy:0.007377 gobierno:0.005269 gracias:0.004957 mañana:0.004615 congreso:0.004615 presidente:0.004264 psoe:0.004235 madrid:0.003708 pp:0.003649 españa:0.003327\n",
      "0.046592\tgracias:0.007856 rajoy:0.005355 año:0.004004 psoe:0.003610 gobierno:0.003540 mañana:0.003530 @telediariointer:0.003025 rubalcaba:0.002975 orbyt:0.002914 feliz:0.002743\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "K=20   # number of topics\n",
    "\n",
    "output_dir=./BTM/output-tweets/\n",
    "model_dir=${output_dir}model/\n",
    "\n",
    "# vocabulary file\n",
    "voca_pt=${output_dir}voca.txt\n",
    "\n",
    "## output top words of each topic\n",
    "echo \"================ Topic Display =============\"\n",
    "python3 ./BTM/script/topicDisplay.py $model_dir $K $voca_pt\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
