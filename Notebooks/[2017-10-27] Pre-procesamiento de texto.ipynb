{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [2017-10-27] Pre-procesamiento de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook se compararán diferentes formas de procesar los documentos antes de analizarlos y realizar consultas sobre ellos. Entre los diferentes métodos de preprocesamiento encontramos stemming, lemmatization y n-grams. Estos 3 métodos se analizarán y probarán a continuación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization con NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar realizaremos un recuento básico del dataset de tweets parseado en el notebook anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('de', 48484), ('la', 32188), ('el', 28392), ('en', 25901), ('a', 24016), ('y', 21301), ('que', 19575), ('los', 12733), ('del', 11224), ('no', 10833)]\n"
     ]
    }
   ],
   "source": [
    "def get_tokens():\n",
    "    with open('data/general-tweets.txt', 'r') as tweets:\n",
    "        text = tweets.read()\n",
    "        lowers = text.lower()\n",
    "        no_punctuation = lowers.translate(str.maketrans('', '', string.punctuation))\n",
    "        tokens = nltk.word_tokenize(no_punctuation)\n",
    "        return tokens\n",
    "\n",
    "tokens = get_tokens()\n",
    "count = Counter(tokens)\n",
    "print(count.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminando Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente preprocesamiento que realizaremos será eliminar las stopwords existentes en el dataset de tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rt', 4795), ('rajoy', 3063), ('gracias', 2243), ('gobierno', 2243), ('psoe', 2009), ('pp', 1887), ('mañana', 1742), ('vía', 1655), ('españa', 1513), ('madrid', 1509)]\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    es_stopwords = [\"a\",\"actualmente\",\"acuerdo\",\"adelante\",\"ademas\",\"además\",\"adrede\",\"afirmó\",\"agregó\",\"ahi\",\"ahora\",\"ahí\",\"al\",\"algo\",\"alguna\",\"algunas\",\"alguno\",\"algunos\",\"algún\",\"alli\",\"allí\",\"alrededor\",\"ambos\",\"ampleamos\",\"antano\",\"antaño\",\"ante\",\"anterior\",\"antes\",\"apenas\",\"aproximadamente\",\"aquel\",\"aquella\",\"aquellas\",\"aquello\",\"aquellos\",\"aqui\",\"aquél\",\"aquélla\",\"aquéllas\",\"aquéllos\",\"aquí\",\"arriba\",\"arribaabajo\",\"aseguró\",\"asi\",\"así\",\"atras\",\"aun\",\"aunque\",\"ayer\",\"añadió\",\"aún\",\"b\",\"bajo\",\"bastante\",\"bien\",\"breve\",\"buen\",\"buena\",\"buenas\",\"bueno\",\"buenos\",\"c\",\"cada\",\"casi\",\"cerca\",\"cierta\",\"ciertas\",\"cierto\",\"ciertos\",\"cinco\",\"claro\",\"comentó\",\"como\",\"con\",\"conmigo\",\"conocer\",\"conseguimos\",\"conseguir\",\"considera\",\"consideró\",\"consigo\",\"consigue\",\"consiguen\",\"consigues\",\"contigo\",\"contra\",\"cosas\",\"creo\",\"cual\",\"cuales\",\"cualquier\",\"cuando\",\"cuanta\",\"cuantas\",\"cuanto\",\"cuantos\",\"cuatro\",\"cuenta\",\"cuál\",\"cuáles\",\"cuándo\",\"cuánta\",\"cuántas\",\"cuánto\",\"cuántos\",\"cómo\",\"d\",\"da\",\"dado\",\"dan\",\"dar\",\"de\",\"debajo\",\"debe\",\"deben\",\"debido\",\"decir\",\"dejó\",\"del\",\"delante\",\"demasiado\",\"demás\",\"dentro\",\"deprisa\",\"desde\",\"despacio\",\"despues\",\"después\",\"detras\",\"detrás\",\"dia\",\"dias\",\"dice\",\"dicen\",\"dicho\",\"dieron\",\"diferente\",\"diferentes\",\"dijeron\",\"dijo\",\"dio\",\"donde\",\"dos\",\"durante\",\"día\",\"días\",\"dónde\",\"e\",\"ejemplo\",\"el\",\"ella\",\"ellas\",\"ello\",\"ellos\",\"embargo\",\"empleais\",\"emplean\",\"emplear\",\"empleas\",\"empleo\",\"en\",\"encima\",\"encuentra\",\"enfrente\",\"enseguida\",\"entonces\",\"entre\",\"era\",\"eramos\",\"eran\",\"eras\",\"eres\",\"es\",\"esa\",\"esas\",\"ese\",\"eso\",\"esos\",\"esta\",\"estaba\",\"estaban\",\"estado\",\"estados\",\"estais\",\"estamos\",\"estan\",\"estar\",\"estará\",\"estas\",\"este\",\"esto\",\"estos\",\"estoy\",\"estuvo\",\"está\",\"están\",\"ex\",\"excepto\",\"existe\",\"existen\",\"explicó\",\"expresó\",\"f\",\"fin\",\"final\",\"fue\",\"fuera\",\"fueron\",\"fui\",\"fuimos\",\"g\",\"general\",\"gran\",\"grandes\",\"gueno\",\"h\",\"ha\",\"haber\",\"habia\",\"habla\",\"hablan\",\"habrá\",\"había\",\"habían\",\"hace\",\"haceis\",\"hacemos\",\"hacen\",\"hacer\",\"hacerlo\",\"haces\",\"hacia\",\"haciendo\",\"hago\",\"han\",\"hasta\",\"hay\",\"haya\",\"he\",\"hecho\",\"hemos\",\"hicieron\",\"hizo\",\"horas\",\"hoy\",\"hubo\",\"i\",\"igual\",\"incluso\",\"indicó\",\"informo\",\"informó\",\"intenta\",\"intentais\",\"intentamos\",\"intentan\",\"intentar\",\"intentas\",\"intento\",\"ir\",\"j\",\"junto\",\"k\",\"l\",\"la\",\"lado\",\"largo\",\"las\",\"le\",\"lejos\",\"les\",\"llegó\",\"lleva\",\"llevar\",\"lo\",\"los\",\"luego\",\"lugar\",\"m\",\"mal\",\"manera\",\"manifestó\",\"mas\",\"mayor\",\"me\",\"mediante\",\"medio\",\"mejor\",\"mencionó\",\"menos\",\"menudo\",\"mi\",\"mia\",\"mias\",\"mientras\",\"mio\",\"mios\",\"mis\",\"misma\",\"mismas\",\"mismo\",\"mismos\",\"modo\",\"momento\",\"mucha\",\"muchas\",\"mucho\",\"muchos\",\"muy\",\"más\",\"mí\",\"mía\",\"mías\",\"mío\",\"míos\",\"n\",\"nada\",\"nadie\",\"ni\",\"ninguna\",\"ningunas\",\"ninguno\",\"ningunos\",\"ningún\",\"no\",\"nos\",\"nosotras\",\"nosotros\",\"nuestra\",\"nuestras\",\"nuestro\",\"nuestros\",\"nueva\",\"nuevas\",\"nuevo\",\"nuevos\",\"nunca\",\"o\",\"ocho\",\"os\",\"otra\",\"otras\",\"otro\",\"otros\",\"p\",\"pais\",\"para\",\"parece\",\"parte\",\"partir\",\"pasada\",\"pasado\",\"paìs\",\"peor\",\"pero\",\"pesar\",\"poca\",\"pocas\",\"poco\",\"pocos\",\"podeis\",\"podemos\",\"poder\",\"podria\",\"podriais\",\"podriamos\",\"podrian\",\"podrias\",\"podrá\",\"podrán\",\"podría\",\"podrían\",\"poner\",\"por\",\"porque\",\"posible\",\"primer\",\"primera\",\"primero\",\"primeros\",\"principalmente\",\"pronto\",\"propia\",\"propias\",\"propio\",\"propios\",\"proximo\",\"próximo\",\"próximos\",\"pudo\",\"pueda\",\"puede\",\"pueden\",\"puedo\",\"pues\",\"q\",\"qeu\",\"que\",\"quedó\",\"queremos\",\"quien\",\"quienes\",\"quiere\",\"quiza\",\"quizas\",\"quizá\",\"quizás\",\"quién\",\"quiénes\",\"qué\",\"r\",\"raras\",\"realizado\",\"realizar\",\"realizó\",\"repente\",\"respecto\",\"s\",\"sabe\",\"sabeis\",\"sabemos\",\"saben\",\"saber\",\"sabes\",\"salvo\",\"se\",\"sea\",\"sean\",\"segun\",\"segunda\",\"segundo\",\"según\",\"seis\",\"ser\",\"sera\",\"será\",\"serán\",\"sería\",\"señaló\",\"si\",\"sido\",\"siempre\",\"siendo\",\"siete\",\"sigue\",\"siguiente\",\"sin\",\"sino\",\"sobre\",\"sois\",\"sola\",\"solamente\",\"solas\",\"solo\",\"solos\",\"somos\",\"son\",\"soy\",\"soyos\",\"su\",\"supuesto\",\"sus\",\"suya\",\"suyas\",\"suyo\",\"sé\",\"sí\",\"sólo\",\"t\",\"tal\",\"tambien\",\"también\",\"tampoco\",\"tan\",\"tanto\",\"tarde\",\"te\",\"temprano\",\"tendrá\",\"tendrán\",\"teneis\",\"tenemos\",\"tener\",\"tenga\",\"tengo\",\"tenido\",\"tenía\",\"tercera\",\"ti\",\"tiempo\",\"tiene\",\"tienen\",\"toda\",\"todas\",\"todavia\",\"todavía\",\"todo\",\"todos\",\"total\",\"trabaja\",\"trabajais\",\"trabajamos\",\"trabajan\",\"trabajar\",\"trabajas\",\"trabajo\",\"tras\",\"trata\",\"través\",\"tres\",\"tu\",\"tus\",\"tuvo\",\"tuya\",\"tuyas\",\"tuyo\",\"tuyos\",\"tú\",\"u\",\"ultimo\",\"un\",\"una\",\"unas\",\"uno\",\"unos\",\"usa\",\"usais\",\"usamos\",\"usan\",\"usar\",\"usas\",\"uso\",\"usted\",\"ustedes\",\"v\",\"va\",\"vais\",\"valor\",\"vamos\",\"van\",\"varias\",\"varios\",\"vaya\",\"veces\",\"ver\",\"verdad\",\"verdadera\",\"verdadero\",\"vez\",\"vosotras\",\"vosotros\",\"voy\",\"vuestra\",\"vuestras\",\"vuestro\",\"vuestros\",\"w\",\"x\",\"y\",\"ya\",\"yo\",\"z\",\"él\",\"ésa\",\"ésas\",\"ése\",\"ésos\",\"ésta\",\"éstas\",\"éste\",\"éstos\",\"última\",\"últimas\",\"último\",\"últimos\",\"“\",\"”\",\" \"]\n",
    "    filtered = [w for w in tokens if not w in es_stopwords]\n",
    "    return filtered\n",
    "\n",
    "tokens = get_tokens()\n",
    "filtered = remove_stopwords(tokens)\n",
    "count = Counter(filtered)\n",
    "print(count.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming es un método para reducir una palabra a su raíz. Stemming aumenta el _recall_ que es una medida sobre el número de documentos que se pueden encontrar con una consulta. Por ejemplo una consulta sobre \"bibliotecas\" también encuentra documentos en los que solo aparezca \"bibliotecario\" porque la raíz (o el _stem_) de las dos palabras es la misma (\"bibliotec\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen diversos algoritmos para realizar stemming. En este caso haremos uso del stemmer Snowball que provee `nltk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora debemos instanciar el stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"spanish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probaremos el stemmer con algunas palabras básicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cas', 'pais', 'aut', 'celular']\n"
     ]
    }
   ],
   "source": [
    "words = [\"casas\", \"países\", \"autos\", \"celulares\"]\n",
    "stemmed = [stemmer.stem(w) for w in words]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora definiremos el método `stem_tokens` que recibe un arreglo de tokens y le aplica stemming a cada uno de ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora definiremos el método `stem_doc` que convertirá el documento en un arreglo de tokens con el stemming aplicado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def stem_doc(text):\n",
    "    lowers = text.lower()\n",
    "    no_punctuation = lowers.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = nltk.word_tokenize(no_punctuation)\n",
    "    filtered = remove_stopwords(tokens)\n",
    "    stems = stem_tokens(filtered, stemmer)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora procederemos a aplicar el stemmer a todos los tokens del dataset importado anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' ', 938355), ('a', 580687), ('e', 553668), ('o', 421975), ('s', 311677), ('n', 309365), ('r', 307695), ('i', 281795), ('t', 252421), ('l', 247144), ('d', 221413), ('c', 197685), ('u', 166381), ('p', 135045), ('m', 123120), ('.', 98433), ('h', 66563), ('/', 66461), ('b', 62580), ('\\n', 60798), ('g', 58679), ('v', 51834), ('y', 50590), (':', 40703), ('q', 38019), ('@', 35951), ('E', 34437), ('f', 32458), (',', 30209), ('j', 29809), ('A', 28859), ('P', 27189), ('z', 26647), ('C', 23874), ('M', 23618), ('R', 22894), ('S', 22500), ('!', 21972), ('í', 21860), ('ó', 20802), ('T', 18146), ('L', 17632), ('á', 17121), ('0', 16123), ('\"', 15935), ('#', 14962), ('G', 14263), ('O', 13082), ('D', 12879), ('B', 12793)]\n"
     ]
    }
   ],
   "source": [
    "with open('data/general-tweets.txt', 'r') as tweets:\n",
    "    text = tweets.read()\n",
    "    \n",
    "stems = stem_doc(text)\n",
    "count = Counter(text)\n",
    "print(count.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora procederemos a cargar varios documentos en memoria. Estos documentos corresponden a tweets cargados de los datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "docs_dict = {}\n",
    "datasets = ['general-tweets', 'politics-tweets']\n",
    "for ds in datasets:\n",
    "    with open(\"data/{}.txt\".format(ds), \"r\") as f:\n",
    "        text = f.read()\n",
    "        lowers = text.lower()\n",
    "        no_punctuation = lowers.translate(str.maketrans('', '', string.punctuation))\n",
    "        docs_dict[ds] = no_punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación calcularemos la métrica tf-idf para cada token posterior a su preprocesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(tokenizer=stem_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfs = tfidf.fit_transform(docs_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este instante tenemos calculados los valores de tf-idf para cada token en cada uno de los dos documentos. Lo siguiente que vamos a realizar es una búsqueda de un nuevo texto sobre ambos documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 58721)\t0.57735026919\n",
      "  (0, 18349)\t0.57735026919\n",
      "  (0, 17681)\t0.57735026919\n"
     ]
    }
   ],
   "source": [
    "query = 'españa es un pais que queda en europa'\n",
    "response = tfidf.transform([query])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos obtenido algunos valores que corresponden a la métrica tf-idf de las palabras de la búsqueda que aparecen en los documentos. Veamos cuáles son estas palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qued  -  0.57735026919\n",
      "europ  -  0.57735026919\n",
      "españ  -  0.57735026919\n"
     ]
    }
   ],
   "source": [
    "feature_names = tfidf.get_feature_names()\n",
    "for col in response.nonzero()[1]:\n",
    "    print(feature_names[col], ' - ', response[0, col])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
