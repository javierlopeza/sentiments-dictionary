{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [2017-10-27] Pre-procesamiento de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook se compararán diferentes formas de procesar los documentos antes de analizarlos y realizar consultas sobre ellos. Entre los diferentes métodos de preprocesamiento encontramos stemming, lemmatization y n-grams. Estos 3 métodos se analizarán y probarán a continuación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization con NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar realizaremos un recuento básico del dataset de tweets parseado en el notebook anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('de', 48484), ('la', 32188), ('el', 28392), ('en', 25901), ('a', 24016), ('y', 21301), ('que', 19575), ('los', 12733), ('del', 11224), ('no', 10833)]\n"
     ]
    }
   ],
   "source": [
    "def get_tokens():\n",
    "    with open('data/general-tweets.txt', 'r') as tweets:\n",
    "        text = tweets.read()\n",
    "        lowers = text.lower()\n",
    "        no_punctuation = lowers.translate(str.maketrans('', '', string.punctuation))\n",
    "        tokens = nltk.word_tokenize(no_punctuation)\n",
    "        return tokens\n",
    "\n",
    "tokens = get_tokens()\n",
    "count = Counter(tokens)\n",
    "print(count.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminando Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente preprocesamiento que realizaremos será eliminar las stopwords existentes en el dataset de tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rt', 4795), ('rajoy', 3063), ('gracias', 2243), ('gobierno', 2243), ('psoe', 2009), ('pp', 1887), ('mañana', 1742), ('vía', 1655), ('españa', 1513), ('madrid', 1509)]\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    es_stopwords = [\"a\",\"actualmente\",\"acuerdo\",\"adelante\",\"ademas\",\"además\",\"adrede\",\"afirmó\",\"agregó\",\"ahi\",\"ahora\",\"ahí\",\"al\",\"algo\",\"alguna\",\"algunas\",\"alguno\",\"algunos\",\"algún\",\"alli\",\"allí\",\"alrededor\",\"ambos\",\"ampleamos\",\"antano\",\"antaño\",\"ante\",\"anterior\",\"antes\",\"apenas\",\"aproximadamente\",\"aquel\",\"aquella\",\"aquellas\",\"aquello\",\"aquellos\",\"aqui\",\"aquél\",\"aquélla\",\"aquéllas\",\"aquéllos\",\"aquí\",\"arriba\",\"arribaabajo\",\"aseguró\",\"asi\",\"así\",\"atras\",\"aun\",\"aunque\",\"ayer\",\"añadió\",\"aún\",\"b\",\"bajo\",\"bastante\",\"bien\",\"breve\",\"buen\",\"buena\",\"buenas\",\"bueno\",\"buenos\",\"c\",\"cada\",\"casi\",\"cerca\",\"cierta\",\"ciertas\",\"cierto\",\"ciertos\",\"cinco\",\"claro\",\"comentó\",\"como\",\"con\",\"conmigo\",\"conocer\",\"conseguimos\",\"conseguir\",\"considera\",\"consideró\",\"consigo\",\"consigue\",\"consiguen\",\"consigues\",\"contigo\",\"contra\",\"cosas\",\"creo\",\"cual\",\"cuales\",\"cualquier\",\"cuando\",\"cuanta\",\"cuantas\",\"cuanto\",\"cuantos\",\"cuatro\",\"cuenta\",\"cuál\",\"cuáles\",\"cuándo\",\"cuánta\",\"cuántas\",\"cuánto\",\"cuántos\",\"cómo\",\"d\",\"da\",\"dado\",\"dan\",\"dar\",\"de\",\"debajo\",\"debe\",\"deben\",\"debido\",\"decir\",\"dejó\",\"del\",\"delante\",\"demasiado\",\"demás\",\"dentro\",\"deprisa\",\"desde\",\"despacio\",\"despues\",\"después\",\"detras\",\"detrás\",\"dia\",\"dias\",\"dice\",\"dicen\",\"dicho\",\"dieron\",\"diferente\",\"diferentes\",\"dijeron\",\"dijo\",\"dio\",\"donde\",\"dos\",\"durante\",\"día\",\"días\",\"dónde\",\"e\",\"ejemplo\",\"el\",\"ella\",\"ellas\",\"ello\",\"ellos\",\"embargo\",\"empleais\",\"emplean\",\"emplear\",\"empleas\",\"empleo\",\"en\",\"encima\",\"encuentra\",\"enfrente\",\"enseguida\",\"entonces\",\"entre\",\"era\",\"eramos\",\"eran\",\"eras\",\"eres\",\"es\",\"esa\",\"esas\",\"ese\",\"eso\",\"esos\",\"esta\",\"estaba\",\"estaban\",\"estado\",\"estados\",\"estais\",\"estamos\",\"estan\",\"estar\",\"estará\",\"estas\",\"este\",\"esto\",\"estos\",\"estoy\",\"estuvo\",\"está\",\"están\",\"ex\",\"excepto\",\"existe\",\"existen\",\"explicó\",\"expresó\",\"f\",\"fin\",\"final\",\"fue\",\"fuera\",\"fueron\",\"fui\",\"fuimos\",\"g\",\"general\",\"gran\",\"grandes\",\"gueno\",\"h\",\"ha\",\"haber\",\"habia\",\"habla\",\"hablan\",\"habrá\",\"había\",\"habían\",\"hace\",\"haceis\",\"hacemos\",\"hacen\",\"hacer\",\"hacerlo\",\"haces\",\"hacia\",\"haciendo\",\"hago\",\"han\",\"hasta\",\"hay\",\"haya\",\"he\",\"hecho\",\"hemos\",\"hicieron\",\"hizo\",\"horas\",\"hoy\",\"hubo\",\"i\",\"igual\",\"incluso\",\"indicó\",\"informo\",\"informó\",\"intenta\",\"intentais\",\"intentamos\",\"intentan\",\"intentar\",\"intentas\",\"intento\",\"ir\",\"j\",\"junto\",\"k\",\"l\",\"la\",\"lado\",\"largo\",\"las\",\"le\",\"lejos\",\"les\",\"llegó\",\"lleva\",\"llevar\",\"lo\",\"los\",\"luego\",\"lugar\",\"m\",\"mal\",\"manera\",\"manifestó\",\"mas\",\"mayor\",\"me\",\"mediante\",\"medio\",\"mejor\",\"mencionó\",\"menos\",\"menudo\",\"mi\",\"mia\",\"mias\",\"mientras\",\"mio\",\"mios\",\"mis\",\"misma\",\"mismas\",\"mismo\",\"mismos\",\"modo\",\"momento\",\"mucha\",\"muchas\",\"mucho\",\"muchos\",\"muy\",\"más\",\"mí\",\"mía\",\"mías\",\"mío\",\"míos\",\"n\",\"nada\",\"nadie\",\"ni\",\"ninguna\",\"ningunas\",\"ninguno\",\"ningunos\",\"ningún\",\"no\",\"nos\",\"nosotras\",\"nosotros\",\"nuestra\",\"nuestras\",\"nuestro\",\"nuestros\",\"nueva\",\"nuevas\",\"nuevo\",\"nuevos\",\"nunca\",\"o\",\"ocho\",\"os\",\"otra\",\"otras\",\"otro\",\"otros\",\"p\",\"pais\",\"para\",\"parece\",\"parte\",\"partir\",\"pasada\",\"pasado\",\"paìs\",\"peor\",\"pero\",\"pesar\",\"poca\",\"pocas\",\"poco\",\"pocos\",\"podeis\",\"podemos\",\"poder\",\"podria\",\"podriais\",\"podriamos\",\"podrian\",\"podrias\",\"podrá\",\"podrán\",\"podría\",\"podrían\",\"poner\",\"por\",\"porque\",\"posible\",\"primer\",\"primera\",\"primero\",\"primeros\",\"principalmente\",\"pronto\",\"propia\",\"propias\",\"propio\",\"propios\",\"proximo\",\"próximo\",\"próximos\",\"pudo\",\"pueda\",\"puede\",\"pueden\",\"puedo\",\"pues\",\"q\",\"qeu\",\"que\",\"quedó\",\"queremos\",\"quien\",\"quienes\",\"quiere\",\"quiza\",\"quizas\",\"quizá\",\"quizás\",\"quién\",\"quiénes\",\"qué\",\"r\",\"raras\",\"realizado\",\"realizar\",\"realizó\",\"repente\",\"respecto\",\"s\",\"sabe\",\"sabeis\",\"sabemos\",\"saben\",\"saber\",\"sabes\",\"salvo\",\"se\",\"sea\",\"sean\",\"segun\",\"segunda\",\"segundo\",\"según\",\"seis\",\"ser\",\"sera\",\"será\",\"serán\",\"sería\",\"señaló\",\"si\",\"sido\",\"siempre\",\"siendo\",\"siete\",\"sigue\",\"siguiente\",\"sin\",\"sino\",\"sobre\",\"sois\",\"sola\",\"solamente\",\"solas\",\"solo\",\"solos\",\"somos\",\"son\",\"soy\",\"soyos\",\"su\",\"supuesto\",\"sus\",\"suya\",\"suyas\",\"suyo\",\"sé\",\"sí\",\"sólo\",\"t\",\"tal\",\"tambien\",\"también\",\"tampoco\",\"tan\",\"tanto\",\"tarde\",\"te\",\"temprano\",\"tendrá\",\"tendrán\",\"teneis\",\"tenemos\",\"tener\",\"tenga\",\"tengo\",\"tenido\",\"tenía\",\"tercera\",\"ti\",\"tiempo\",\"tiene\",\"tienen\",\"toda\",\"todas\",\"todavia\",\"todavía\",\"todo\",\"todos\",\"total\",\"trabaja\",\"trabajais\",\"trabajamos\",\"trabajan\",\"trabajar\",\"trabajas\",\"trabajo\",\"tras\",\"trata\",\"través\",\"tres\",\"tu\",\"tus\",\"tuvo\",\"tuya\",\"tuyas\",\"tuyo\",\"tuyos\",\"tú\",\"u\",\"ultimo\",\"un\",\"una\",\"unas\",\"uno\",\"unos\",\"usa\",\"usais\",\"usamos\",\"usan\",\"usar\",\"usas\",\"uso\",\"usted\",\"ustedes\",\"v\",\"va\",\"vais\",\"valor\",\"vamos\",\"van\",\"varias\",\"varios\",\"vaya\",\"veces\",\"ver\",\"verdad\",\"verdadera\",\"verdadero\",\"vez\",\"vosotras\",\"vosotros\",\"voy\",\"vuestra\",\"vuestras\",\"vuestro\",\"vuestros\",\"w\",\"x\",\"y\",\"ya\",\"yo\",\"z\",\"él\",\"ésa\",\"ésas\",\"ése\",\"ésos\",\"ésta\",\"éstas\",\"éste\",\"éstos\",\"última\",\"últimas\",\"último\",\"últimos\",\"“\",\"”\",\" \"]\n",
    "    filtered = [w for w in tokens if not w in es_stopwords]\n",
    "    return filtered\n",
    "\n",
    "tokens = get_tokens()\n",
    "filtered = remove_stopwords(tokens)\n",
    "count = Counter(filtered)\n",
    "print(count.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming es un método para reducir una palabra a su raíz. Stemming aumenta el _recall_ que es una medida sobre el número de documentos que se pueden encontrar con una consulta. Por ejemplo una consulta sobre \"bibliotecas\" también encuentra documentos en los que solo aparezca \"bibliotecario\" porque la raíz (o el _stem_) de las dos palabras es la misma (\"bibliotec\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen diversos algoritmos para realizar stemming. En este caso haremos uso del stemmer Snowball que provee `nltk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora debemos instanciar el stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"spanish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probaremos el stemmer con algunas palabras básicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cas', 'pais', 'aut', 'celular']\n"
     ]
    }
   ],
   "source": [
    "words = [\"casas\", \"países\", \"autos\", \"celulares\"]\n",
    "stemmed = [stemmer.stem(w) for w in words]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora definiremos el método `stem_tokens` que recibe un arreglo de tokens y le aplica stemming a cada uno de ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora definiremos el método `stem_doc` que convertirá el documento en un arreglo de tokens con el stemming aplicado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def stem_doc(text):\n",
    "    lowers = text.lower()\n",
    "    no_punctuation = lowers.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = nltk.word_tokenize(no_punctuation)\n",
    "    filtered = remove_stopwords(tokens)\n",
    "    stems = stem_tokens(filtered, stemmer)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora procederemos a aplicar el stemmer a todos los tokens del dataset importado anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rt', 4795), ('rajoy', 3066), ('via', 2489), ('gobiern', 2313), ('graci', 2304), ('pso', 2009), ('pp', 1887), ('mañan', 1765), ('madr', 1674), ('public', 1641), ('españ', 1522), ('congres', 1387), ('reform', 1267), ('polit', 1237), ('esper', 1227), ('noch', 1223), ('andaluc', 1206), ('rubalc', 1168), ('port', 1164), ('dej', 1129), ('cas', 1110), ('años', 1102), ('pas', 1070), ('social', 1040), ('ministr', 1023), ('acab', 1015), ('president', 1006), ('telediarioint', 971), ('cont', 952), ('año', 952), ('habl', 944), ('par', 928), ('malag', 927), ('period', 924), ('gan', 923), ('sal', 911), ('cambi', 904), ('part', 895), ('lleg', 887), ('amig', 882), ('recort', 876), ('2030', 845), ('español', 842), ('millon', 824), ('gust', 804), ('vot', 785), ('mariviromer', 780), ('elcambioandaluz', 774), ('qued', 752), ('pag', 751)]\n"
     ]
    }
   ],
   "source": [
    "with open('data/general-tweets.txt', 'r') as tweets:\n",
    "    text = tweets.read()\n",
    "    \n",
    "stems = stem_doc(text)\n",
    "count = Counter(stems)\n",
    "print(count.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora procederemos a cargar varios documentos en memoria. Estos documentos corresponden a tweets cargados de los datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "docs_dict = {}\n",
    "datasets = ['general-tweets', 'politics-tweets']\n",
    "for ds in datasets:\n",
    "    with open(\"data/{}.txt\".format(ds), \"r\") as f:\n",
    "        text = f.read()\n",
    "        lowers = text.lower()\n",
    "        no_punctuation = lowers.translate(str.maketrans('', '', string.punctuation))\n",
    "        docs_dict[ds] = no_punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación calcularemos la métrica tf-idf para cada token posterior a su preprocesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(tokenizer=stem_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfs = tfidf.fit_transform(docs_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este instante tenemos calculados los valores de tf-idf para cada token en cada uno de los dos documentos. Lo siguiente que vamos a realizar es una búsqueda de un nuevo texto sobre ambos documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 58721)\t0.57735026919\n",
      "  (0, 18349)\t0.57735026919\n",
      "  (0, 17681)\t0.57735026919\n"
     ]
    }
   ],
   "source": [
    "query = 'españa es un pais que queda en europa'\n",
    "response = tfidf.transform([query])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos obtenido algunos valores que corresponden a la métrica tf-idf de las palabras de la búsqueda que aparecen en los documentos. Veamos cuáles son estas palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qued  -  0.57735026919\n",
      "europ  -  0.57735026919\n",
      "españ  -  0.57735026919\n"
     ]
    }
   ],
   "source": [
    "feature_names = tfidf.get_feature_names()\n",
    "for col in response.nonzero()[1]:\n",
    "    print(feature_names[col], ' - ', response[0, col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora procederemos a analizar otro tipo de pre-procesamiento de texto denominado _Lemmatization_. Este es un proceso lingüístico que consiste en hallar el lema correspondiente a una _forma flexionada_ dada. Una forma flexionada corresponde, por ejemplo, al plural, femenino o forma conjugada de una palabra. El lema es la forma que por convenio se acepta como representante de todas las formas flexionadas de una misma palabra. Por ejemplo, el lema de _dije_, _diré_ y _diremos_ es _decir_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La diferencia entre _lemmatization_ y _stemming_ es que el stemmer opera sobre una palabra individual sin conocer el contexto, y por lo tanto no puede discriminar entre palabras que pueden tener diferente significado dependiendo de la forma en que se usa dentro de la oración."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lamentablemente la librería `nltk` no permite aplicar _lemmatization_ en español. Para solucionar esto ocuparemos nuestro propio diccionario de lemas obtenido desde http://www.lexiconista.com/Datasets/lemmatization-es.zip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmaDict = {}\n",
    "with open('lemmatization-es.txt', 'rb') as f:\n",
    "    data = f.read().decode('utf8').replace(u'\\r', u'').split(u'\\n')\n",
    "    data = [a.split(u'\\t') for a in data]\n",
    "\n",
    "for a in data:\n",
    "    if len(a) > 1:\n",
    "        lemmaDict[a[1]] = a[0]\n",
    "\n",
    "def lemmatize(word):\n",
    "    return lemmaDict.get(word, word + u'*')\n",
    "   \n",
    "def get_tokens(doc):\n",
    "    with open(doc, \"r\") as f:\n",
    "        text = f.read()\n",
    "        tokens = nltk.word_tokenize(text.lower().translate(str.maketrans('', '', string.punctuation)))\n",
    "    return tokens\n",
    "        \n",
    "tokens = get_tokens('data/general-tweets.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos cómo se aplicó _lemmatization_ a las primeras 50 palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original   ->   Lema\n",
      "--------------------\n",
      "portada   ->   portar\n",
      "público   ->   público*\n",
      "viernes   ->   viernes*\n",
      "fabra   ->   fabra*\n",
      "al   ->   al*\n",
      "banquillo   ->   banquillo*\n",
      "por   ->   por*\n",
      "orden   ->   orden*\n",
      "del   ->   del*\n",
      "supremo   ->   supremo*\n",
      "wikileaks   ->   wikileaks*\n",
      "retrata   ->   retratar\n",
      "a   ->   a*\n",
      "160   ->   160*\n",
      "empresas   ->   empresa\n",
      "espías   ->   espiar\n",
      "httptcoytpru0fd   ->   httptcoytpru0fd*\n",
      "grande   ->   grande*\n",
      "rt   ->   rt*\n",
      "veronicacalderon   ->   veronicacalderon*\n",
      "el   ->   el*\n",
      "periodista   ->   periodista*\n",
      "es   ->   ser\n",
      "alguien   ->   alguien*\n",
      "que   ->   que*\n",
      "quiere   ->   querer\n",
      "contar   ->   contar*\n",
      "la   ->   lo\n",
      "realidad   ->   realidad*\n",
      "pero   ->   pero*\n",
      "no   ->   no*\n",
      "vive   ->   vivir\n",
      "en   ->   en*\n",
      "ella   ->   él\n",
      "via   ->   via*\n",
      "galtares   ->   galtares*\n",
      "gonzalo   ->   gonzalo*\n",
      "altozano   ->   altozano*\n",
      "tras   ->   tras*\n",
      "la   ->   lo\n",
      "presentación   ->   presentación*\n",
      "de   ->   de*\n",
      "su   ->   su*\n",
      "libro   ->   librar\n",
      "101   ->   101*\n",
      "españoles   ->   españolar\n",
      "y   ->   y*\n",
      "dios   ->   dios*\n",
      "divertido   ->   divertir\n",
      "emocionante   ->   emocionante*\n"
     ]
    }
   ],
   "source": [
    "lemas = []\n",
    "print(\"Original   ->   Lema\")\n",
    "print(\"--------------------\")\n",
    "for word in tokens[:50]:\n",
    "    word_lema = lemmatize(word)\n",
    "    print(\"{}   ->   {}\".format(word, word_lema))\n",
    "for word in tokens:\n",
    "    word_lema = lemmatize(word)\n",
    "    lemas.append(word_lema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos claramente cómo palabras como _divertido_ se transforman en _divertir_ (su lema). Las palabras señaladas con * en su término significa que no cambiaron respecto a su forma original debido a que no existen en la lista de lemas o corresponden al lema mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('de*', 48484), ('lo', 44921), ('el*', 28392), ('en*', 25901), ('a*', 24016), ('y*', 21301), ('que*', 19575), ('uno', 14837), ('ser', 13857), ('del*', 11224), ('no*', 10833), ('con*', 9329), ('haber', 8905), ('parir', 8780), ('los', 7775), ('se*', 7388), ('por*', 7368), ('el', 6096), ('estar', 4973), ('q*', 4842), ('al*', 4815), ('rt*', 4795), ('“*', 4573), ('me*', 4503), ('”*', 4362), ('hoy*', 3508), ('tener', 3500), ('su*', 3447), ('este', 3378), ('ir', 3282), ('d*', 3281), ('comer', 3157), ('rajoy*', 3063), ('decir', 3056), ('bueno', 2987), ('más*', 2966), ('mi*', 2891), ('todo', 2887), ('si*', 2779), ('hacer', 2738), ('ya*', 2560), ('sobrar', 2430), ('gobernar', 2374), ('pero*', 2334), ('poder', 2265), ('gracia', 2243), ('psoe*', 2009), ('pp*', 1887), ('le*', 1795), ('mañana*', 1742)]\n"
     ]
    }
   ],
   "source": [
    "count = Counter(lemas)\n",
    "print(count.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este último paso obtuvimos las 50 palabras (o tokens) más frecuentes posterior a la aplicación de _lemmatization_. Notamos la diferencia de palabras más frecuentes en comparación con la aplicación del _stemming_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El próximo método de preprocesamiento será aplicar _n-grams_ a los documentos. Un _n-gram_ (o n_grama) es una subsecuencia de n elementos de una secuencia dada. Para esto usaremos `nltk` que provee la ufncionalidad de generar bigramas, trigramas, ..., n-gramas que necesitemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('La', 'Universidad'), ('Universidad', 'Católica'), ('Católica', 'es'), ('es', 'una'), ('una', 'prestigiosa'), ('prestigiosa', 'universidad'), ('universidad', 'latinoamericana'), ('latinoamericana', '.'), ('.', 'La'), ('La', 'Universidad'), ('Universidad', 'Católica'), ('Católica', 'posee'), ('posee', 'campus'), ('campus', 'de'), ('de', 'estudio'), ('estudio', 'en'), ('en', 'diferentes'), ('diferentes', 'regiones'), ('regiones', 'del'), ('del', 'país'), ('país', '.')]\n"
     ]
    }
   ],
   "source": [
    "text = \"La Universidad Católica es una prestigiosa universidad latinoamericana. La Universidad Católica posee campus de estudio en diferentes regiones del país.\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notemos que al aplicar un bigrama sobre el texto ingresado, nos entrega todas las subsecuencias de largo 2 que se pueden generar del texto. Veamos la frecuencia de estos bigramas en el texto original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({('La', 'Universidad'): 2, ('Universidad', 'Católica'): 2, ('Católica', 'es'): 1, ('es', 'una'): 1, ('una', 'prestigiosa'): 1, ('prestigiosa', 'universidad'): 1, ('universidad', 'latinoamericana'): 1, ('latinoamericana', '.'): 1, ('.', 'La'): 1, ('Católica', 'posee'): 1, ('posee', 'campus'): 1, ('campus', 'de'): 1, ('de', 'estudio'): 1, ('estudio', 'en'): 1, ('en', 'diferentes'): 1, ('diferentes', 'regiones'): 1, ('regiones', 'del'): 1, ('del', 'país'): 1, ('país', '.'): 1})\n"
     ]
    }
   ],
   "source": [
    "count = Counter(bigrams)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver cómo bigramas como \"La Universidad\", o \"Universidad Católica\" son más recurrentes en el texto. Ahora veamos qué obtenemos para el dataset de tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = get_tokens('data/general-tweets.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = list(ngrams(tokens, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('de', 'la'), 5860), (('en', 'el'), 4292), (('en', 'la'), 3136), (('a', 'la'), 2822), (('de', 'los'), 2457), (('a', 'los'), 1797), (('a', 'las'), 1527), (('de', 'las'), 1299), (('lo', 'que'), 1242), (('que', 'no'), 1174), (('con', 'el'), 1099), (('a', 'todos'), 1070), (('y', 'el'), 1042), (('el', 'gobierno'), 1006), (('y', 'la'), 931), (('que', 'el'), 913), (('buenos', 'días'), 910), (('con', 'la'), 890), (('que', 'se'), 889), (('no', 'se'), 884), (('telediariointer', '2030'), 802), (('no', 'es'), 780), (('por', 'la'), 766), (('por', 'el'), 713), (('el', 'pp'), 692), (('del', 'pp'), 686), (('el', 'psoe'), 682), (('que', 'la'), 678), (('es', 'un'), 657), (('es', 'el'), 651), (('es', 'la'), 648), (('para', 'el'), 635), (('todos', 'los'), 617), (('la', 'reforma'), 611), (('del', 'psoe'), 609), (('los', 'que'), 603), (('en', 'los'), 587), (('y', 'a'), 587), (('va', 'a'), 574), (('de', 'un'), 570), (('con', 'los'), 542), (('reforma', 'laboral'), 541), (('gracias', 'a'), 534), (('del', 'gobierno'), 520), (('y', 'no'), 519), (('en', 'un'), 517), (('sobre', 'el'), 507), (('de', 'rajoy'), 499), (('buenas', 'noches'), 496), (('dice', 'que'), 495)]\n"
     ]
    }
   ],
   "source": [
    "count = Counter(bigrams)\n",
    "print(count.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es notable cómo bigramas como \"reforma laboral\" o \"buenas noches\" son recurrentes dentro del dataset. Ahora veamos qué ocurre cuando analizamos los trigramas del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = list(ngrams(tokens, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('la', 'reforma', 'laboral'), 390), (('en', 'el', 'congreso'), 349), (('buenos', 'días', 'a'), 267), (('días', 'a', 'todos'), 229), (('a', 'todos', 'los'), 221), (('gracias', 'a', 'todos'), 209), (('a', 'las', '3'), 208), (('rueda', 'de', 'prensa'), 179), (('la', 'portada', 'de'), 169), (('a', 'ver', 'si'), 167), (('a', 'punto', 'de'), 166), (('de', 'la', 'junta'), 165), (('las', '3', 'en'), 160), (('▸', 'historias', 'del'), 160), (('historias', 'del', 'día'), 160), (('del', 'día', 'por'), 160), (('de', 'lo', 'que'), 149), (('que', 'no', 'se'), 143), (('millones', 'de', 'euros'), 142), (('a', 'partir', 'de'), 140), (('3', 'en', 'a3noticias'), 139), (('a', 'los', 'que'), 137), (('en', 'el', 'telediariointer'), 136), (('el', 'telediariointer', '2030'), 134), (('la', 'ley', 'de'), 133), (('la', 'junta', 'de'), 132), (('me', 'voy', 'a'), 129), (('uno', 'de', 'los'), 128), (('consejo', 'de', 'ministros'), 127), (('esta', 'es', 'la'), 125), (('en', 'el', 'blog'), 123), (('portada', 'de', 'hoy'), 123), (('de', 'la', 'crisis'), 120), (('¡noticias', 'descombacantes', 'está'), 118), (('descombacantes', 'está', 'disponible'), 118), (('mejor', 'de', 'mi'), 114), (('¡lo', 'mejor', 'de'), 113), (('de', 'mi', 'timeline'), 113), (('mi', 'timeline', 'está'), 113), (('timeline', 'está', 'disponible'), 113), (('está', 'disponible', 'httptcopdwfyyra'), 112), (('de', 'la', 'reforma'), 110), (('en', 'el', 'psoe'), 107), (('el', 'gobierno', 'de'), 105), (('va', 'a', 'ser'), 101), (('fin', 'de', 'semana'), 100), (('no', 'se', 'puede'), 99), (('es', 'lo', 'que'), 99), (('partir', 'de', 'las'), 97), (('todos', 'los', 'que'), 96)]\n"
     ]
    }
   ],
   "source": [
    "count = Counter(trigrams)\n",
    "print(count.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso obtenemos sets más significativos aún. El trigrama más frecuente es \"la reforma laboral\", seguido de \"en el congreso\". En este instante podemos procesar estos sets de tokens como unidades gramaticales para futuro análisis estadístico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente paso es ver qué resultados se obtienen cuando procesamos los 4-gramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourgrams = list(ngrams(tokens, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('buenos', 'días', 'a', 'todos'), 228), (('▸', 'historias', 'del', 'día'), 160), (('historias', 'del', 'día', 'por'), 160), (('a', 'las', '3', 'en'), 158), (('las', '3', 'en', 'a3noticias'), 139), (('la', 'portada', 'de', 'hoy'), 122), (('¡noticias', 'descombacantes', 'está', 'disponible'), 118), (('en', 'el', 'telediariointer', '2030'), 116), (('¡lo', 'mejor', 'de', 'mi'), 113), (('mejor', 'de', 'mi', 'timeline'), 113), (('de', 'mi', 'timeline', 'está'), 113), (('mi', 'timeline', 'está', 'disponible'), 113), (('descombacantes', 'está', 'disponible', 'httptcopdwfyyra'), 112), (('la', 'junta', 'de', 'andalucía'), 94), (('a', 'partir', 'de', 'las'), 93), (('timeline', 'está', 'disponible', 'httptco0l0hzkdb'), 84), (('está', 'disponible', 'httptco0l0hzkdb', '▸'), 81), (('disponible', 'httptco0l0hzkdb', '▸', 'historias'), 81), (('httptco0l0hzkdb', '▸', 'historias', 'del'), 81), (('de', 'la', 'reforma', 'laboral'), 75), (('la', 'rueda', 'de', 'prensa'), 75), (('la', 'prima', 'de', 'riesgo'), 73), (('a', 'todos', 'los', 'que'), 71), (('buenas', 'noches', 'a', 'todos'), 67), (('gracias', 'a', 'todos', 'por'), 64), (('es', 'la', 'portada', 'de'), 63), (('esta', 'es', 'la', 'portada'), 61), (('de', 'la', 'junta', 'de'), 55), (('lo', 'contamos', 'en', 'el'), 55), (('contamos', 'en', 'el', 'telediariointer'), 55), (('a', 'punto', 'de', 'empezar'), 51), (('gracias', 'a', 'todos', 'los'), 51), (('muy', 'buenas', 'noches', 'followercetes'), 50), (('hoy', 'm', 'acompañan', 'en'), 50), (('os', 'lo', 'contamos', 'en'), 50), (('está', 'disponible', 'httptcopdwfyyra', '▸'), 49), (('disponible', 'httptcopdwfyyra', '▸', 'historias'), 49), (('httptcopdwfyyra', '▸', 'historias', 'del'), 49), (('el', 'consejo', 'de', 'ministros'), 48), (('días', 'a', 'todos', 'así'), 48), (('a', 'todos', 'así', 'amanece'), 48), (('la', 'subida', 'de', 'impuestos'), 48), (('todos', 'así', 'amanece', 'madrid'), 47), (('hoy', 'es', 'el', 'día'), 46), (('rueda', 'de', 'prensa', 'de'), 45), (('aquí', 'os', 'dejo', 'la'), 43), (('telediariointer', 'a', 'las', '3'), 43), (('la', 'mesa', 'del', 'congreso'), 43), (('os', 'dejo', 'la', 'portada'), 40), (('dejo', 'la', 'portada', 'de'), 40)]\n"
     ]
    }
   ],
   "source": [
    "count = Counter(fourgrams)\n",
    "print(count.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí ya se comienzan a obtener resultados diferentes a los anteriores claramente. Del análisis anterior podemos deducir que los n-gramas hay que tratarlos con rigurosidad porque para cierto n esta metodología deja de ser de gran utilidad. Con un n muy grande algunos sets pueden no tener sentido, como también pueden ser los sets ideales para tratar conjuntos y subsecuencias de tokens como unidades gramaticales. Por ejemplo, \"Pontificia Universidad Católica de Chile\" sería un excelente 5-grama."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
