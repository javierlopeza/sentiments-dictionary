{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [2017-10-13] Get started with gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzamos por parsear los tweets encontrados en `general-tweets-test-tagged.xml` y extraer de ellos solamente el contenido. Cada tweet corresponderá a una línea del archivo `tweets.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "tweets = ET.parse('data/general-tweets-test-tagged.xml').getroot()\n",
    "\n",
    "with open(\"data/tweets.txt\", \"a+\") as f:\n",
    "\tfor tweet in tweets:\n",
    "\t\tcontent = tweet.findall('content')[0].text\n",
    "\t\tf.write(\"{}\\n\".format(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto obtenemos un archivo en el siguiente formato:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Más de mañana en Gaceta. UPyD contará casi seguro con grupo gracias al Foro Asturias. Eso se dice en el Congreso\n",
    "Muchas gracias a todos por los comentarios durante el programa de hoy en #VeoTV @MundoaCuestas :-))\n",
    "Habia prometido responder a todos, pero me ha sido imposible. Y hoy no doy para mas. MUCHAS GRACIAS A TODOS\n",
    "\"Soy de ese tipo de personas que no acaban de comprender las cosas hasta que las ponen por escrito\" Murakami\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comenzar trabajando con `gensim` importamos `corpora` que nos permitirá generar un diccionario a partir de los tweets parseados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El archivo de tweets es manejable en memoria por lo que lo cargaremos en una lista de tweets. Consideraremos un documento por tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/tweets.txt\", \"r\") as f:\n",
    "    documents = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para preprocesar los documentos, comenzaremos por procesar las stopwords. Obviamente necesitamos stopwords en español y se han obtenido de https://github.com/6/stopwords-json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = [\"a\",\"actualmente\",\"acuerdo\",\"adelante\",\"ademas\",\"además\",\"adrede\",\"afirmó\",\"agregó\",\"ahi\",\"ahora\",\"ahí\",\"al\",\"algo\",\"alguna\",\"algunas\",\"alguno\",\"algunos\",\"algún\",\"alli\",\"allí\",\"alrededor\",\"ambos\",\"ampleamos\",\"antano\",\"antaño\",\"ante\",\"anterior\",\"antes\",\"apenas\",\"aproximadamente\",\"aquel\",\"aquella\",\"aquellas\",\"aquello\",\"aquellos\",\"aqui\",\"aquél\",\"aquélla\",\"aquéllas\",\"aquéllos\",\"aquí\",\"arriba\",\"arribaabajo\",\"aseguró\",\"asi\",\"así\",\"atras\",\"aun\",\"aunque\",\"ayer\",\"añadió\",\"aún\",\"b\",\"bajo\",\"bastante\",\"bien\",\"breve\",\"buen\",\"buena\",\"buenas\",\"bueno\",\"buenos\",\"c\",\"cada\",\"casi\",\"cerca\",\"cierta\",\"ciertas\",\"cierto\",\"ciertos\",\"cinco\",\"claro\",\"comentó\",\"como\",\"con\",\"conmigo\",\"conocer\",\"conseguimos\",\"conseguir\",\"considera\",\"consideró\",\"consigo\",\"consigue\",\"consiguen\",\"consigues\",\"contigo\",\"contra\",\"cosas\",\"creo\",\"cual\",\"cuales\",\"cualquier\",\"cuando\",\"cuanta\",\"cuantas\",\"cuanto\",\"cuantos\",\"cuatro\",\"cuenta\",\"cuál\",\"cuáles\",\"cuándo\",\"cuánta\",\"cuántas\",\"cuánto\",\"cuántos\",\"cómo\",\"d\",\"da\",\"dado\",\"dan\",\"dar\",\"de\",\"debajo\",\"debe\",\"deben\",\"debido\",\"decir\",\"dejó\",\"del\",\"delante\",\"demasiado\",\"demás\",\"dentro\",\"deprisa\",\"desde\",\"despacio\",\"despues\",\"después\",\"detras\",\"detrás\",\"dia\",\"dias\",\"dice\",\"dicen\",\"dicho\",\"dieron\",\"diferente\",\"diferentes\",\"dijeron\",\"dijo\",\"dio\",\"donde\",\"dos\",\"durante\",\"día\",\"días\",\"dónde\",\"e\",\"ejemplo\",\"el\",\"ella\",\"ellas\",\"ello\",\"ellos\",\"embargo\",\"empleais\",\"emplean\",\"emplear\",\"empleas\",\"empleo\",\"en\",\"encima\",\"encuentra\",\"enfrente\",\"enseguida\",\"entonces\",\"entre\",\"era\",\"eramos\",\"eran\",\"eras\",\"eres\",\"es\",\"esa\",\"esas\",\"ese\",\"eso\",\"esos\",\"esta\",\"estaba\",\"estaban\",\"estado\",\"estados\",\"estais\",\"estamos\",\"estan\",\"estar\",\"estará\",\"estas\",\"este\",\"esto\",\"estos\",\"estoy\",\"estuvo\",\"está\",\"están\",\"ex\",\"excepto\",\"existe\",\"existen\",\"explicó\",\"expresó\",\"f\",\"fin\",\"final\",\"fue\",\"fuera\",\"fueron\",\"fui\",\"fuimos\",\"g\",\"general\",\"gran\",\"grandes\",\"gueno\",\"h\",\"ha\",\"haber\",\"habia\",\"habla\",\"hablan\",\"habrá\",\"había\",\"habían\",\"hace\",\"haceis\",\"hacemos\",\"hacen\",\"hacer\",\"hacerlo\",\"haces\",\"hacia\",\"haciendo\",\"hago\",\"han\",\"hasta\",\"hay\",\"haya\",\"he\",\"hecho\",\"hemos\",\"hicieron\",\"hizo\",\"horas\",\"hoy\",\"hubo\",\"i\",\"igual\",\"incluso\",\"indicó\",\"informo\",\"informó\",\"intenta\",\"intentais\",\"intentamos\",\"intentan\",\"intentar\",\"intentas\",\"intento\",\"ir\",\"j\",\"junto\",\"k\",\"l\",\"la\",\"lado\",\"largo\",\"las\",\"le\",\"lejos\",\"les\",\"llegó\",\"lleva\",\"llevar\",\"lo\",\"los\",\"luego\",\"lugar\",\"m\",\"mal\",\"manera\",\"manifestó\",\"mas\",\"mayor\",\"me\",\"mediante\",\"medio\",\"mejor\",\"mencionó\",\"menos\",\"menudo\",\"mi\",\"mia\",\"mias\",\"mientras\",\"mio\",\"mios\",\"mis\",\"misma\",\"mismas\",\"mismo\",\"mismos\",\"modo\",\"momento\",\"mucha\",\"muchas\",\"mucho\",\"muchos\",\"muy\",\"más\",\"mí\",\"mía\",\"mías\",\"mío\",\"míos\",\"n\",\"nada\",\"nadie\",\"ni\",\"ninguna\",\"ningunas\",\"ninguno\",\"ningunos\",\"ningún\",\"no\",\"nos\",\"nosotras\",\"nosotros\",\"nuestra\",\"nuestras\",\"nuestro\",\"nuestros\",\"nueva\",\"nuevas\",\"nuevo\",\"nuevos\",\"nunca\",\"o\",\"ocho\",\"os\",\"otra\",\"otras\",\"otro\",\"otros\",\"p\",\"pais\",\"para\",\"parece\",\"parte\",\"partir\",\"pasada\",\"pasado\",\"paìs\",\"peor\",\"pero\",\"pesar\",\"poca\",\"pocas\",\"poco\",\"pocos\",\"podeis\",\"podemos\",\"poder\",\"podria\",\"podriais\",\"podriamos\",\"podrian\",\"podrias\",\"podrá\",\"podrán\",\"podría\",\"podrían\",\"poner\",\"por\",\"porque\",\"posible\",\"primer\",\"primera\",\"primero\",\"primeros\",\"principalmente\",\"pronto\",\"propia\",\"propias\",\"propio\",\"propios\",\"proximo\",\"próximo\",\"próximos\",\"pudo\",\"pueda\",\"puede\",\"pueden\",\"puedo\",\"pues\",\"q\",\"qeu\",\"que\",\"quedó\",\"queremos\",\"quien\",\"quienes\",\"quiere\",\"quiza\",\"quizas\",\"quizá\",\"quizás\",\"quién\",\"quiénes\",\"qué\",\"r\",\"raras\",\"realizado\",\"realizar\",\"realizó\",\"repente\",\"respecto\",\"s\",\"sabe\",\"sabeis\",\"sabemos\",\"saben\",\"saber\",\"sabes\",\"salvo\",\"se\",\"sea\",\"sean\",\"segun\",\"segunda\",\"segundo\",\"según\",\"seis\",\"ser\",\"sera\",\"será\",\"serán\",\"sería\",\"señaló\",\"si\",\"sido\",\"siempre\",\"siendo\",\"siete\",\"sigue\",\"siguiente\",\"sin\",\"sino\",\"sobre\",\"sois\",\"sola\",\"solamente\",\"solas\",\"solo\",\"solos\",\"somos\",\"son\",\"soy\",\"soyos\",\"su\",\"supuesto\",\"sus\",\"suya\",\"suyas\",\"suyo\",\"sé\",\"sí\",\"sólo\",\"t\",\"tal\",\"tambien\",\"también\",\"tampoco\",\"tan\",\"tanto\",\"tarde\",\"te\",\"temprano\",\"tendrá\",\"tendrán\",\"teneis\",\"tenemos\",\"tener\",\"tenga\",\"tengo\",\"tenido\",\"tenía\",\"tercera\",\"ti\",\"tiempo\",\"tiene\",\"tienen\",\"toda\",\"todas\",\"todavia\",\"todavía\",\"todo\",\"todos\",\"total\",\"trabaja\",\"trabajais\",\"trabajamos\",\"trabajan\",\"trabajar\",\"trabajas\",\"trabajo\",\"tras\",\"trata\",\"través\",\"tres\",\"tu\",\"tus\",\"tuvo\",\"tuya\",\"tuyas\",\"tuyo\",\"tuyos\",\"tú\",\"u\",\"ultimo\",\"un\",\"una\",\"unas\",\"uno\",\"unos\",\"usa\",\"usais\",\"usamos\",\"usan\",\"usar\",\"usas\",\"uso\",\"usted\",\"ustedes\",\"v\",\"va\",\"vais\",\"valor\",\"vamos\",\"van\",\"varias\",\"varios\",\"vaya\",\"veces\",\"ver\",\"verdad\",\"verdadera\",\"verdadero\",\"vez\",\"vosotras\",\"vosotros\",\"voy\",\"vuestra\",\"vuestras\",\"vuestro\",\"vuestros\",\"w\",\"x\",\"y\",\"ya\",\"yo\",\"z\",\"él\",\"ésa\",\"ésas\",\"ése\",\"ésos\",\"ésta\",\"éstas\",\"éste\",\"éstos\",\"última\",\"últimas\",\"último\",\"últimos\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos las stopwords de cada documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "         for document in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro preprocesamiento básico que realizaremos es eliminar las palabras que aparecen una sola vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [[token for token in text if frequency[token] > 1]\n",
    "         for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actualmente tenemos una lista con listas de los tokens de cada documento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "texts = [['portada',\n",
    "  \"'público',\",\n",
    "  'viernes.',\n",
    "  'fabra',\n",
    "  'banquillo',\n",
    "  'supremo;',\n",
    "  'wikileaks',\n",
    "  '160',\n",
    "  'empresas'],\n",
    " ['grande!',\n",
    "  'rt',\n",
    "  '\"el',\n",
    "  'periodista',\n",
    "  'alguien',\n",
    "  'contar',\n",
    "  'realidad,',\n",
    "  'vive',\n",
    "  'ella\"',\n",
    "  'via',\n",
    "  '@galtares'],\n",
    "  ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que aún se podrían hacer más preprocesamientos, como eliminar los signos de puntuación, comillas, etc. Para efectos de este notebook no se procesarán más, dado que es un acercamiento simple a lo que permite realizar `gensim`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora crearemos un diccionario con los textos encontrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos revisar el diccionario que ha sido creado. Al ejecutar `print(dictionary.token2id)` veremos los ids que le ha asignado a cada token, y el diccionario que hemos generado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "{'portada': 0, \"'público',\": 1, 'viernes.': 2, 'fabra': 3, 'banquillo': 4, 'supremo;': 5, 'wikileaks': 6, '160': 7, 'empresas': 8, 'grande!': 9, 'rt': 10, '\"el': 11, 'periodista': 12, 'alguien': 13, 'contar': 14, 'realidad,': 15, 'vive': 16, 'ella\"': 17, 'via': 18, '@galtares': 19, 'gonzalo': 20, 'altozano': 21, 'presentación': 22, 'libro': 23, '101': 24, 'españoles': 25, 'dios.': 26, 'divertido,': 27, 'emocionante': 28, ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora procederemos a calcular el vector bag of words de un nuevo documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(28, 1), (361, 1), (1313, 1)]\n"
     ]
    }
   ],
   "source": [
    "new_doc = \"El partido del viernes fue muy emocionante\"\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "print(new_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisemos a qué palabras corresponden los ids 28, 361 y 1313."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1313\n",
      "361\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id[\"partido\"])\n",
    "print(dictionary.token2id[\"viernes\"])\n",
    "print(dictionary.token2id[\"emocionante\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que efectivamente corresponden a las 3 palabras principales del nuevo documento. Es de esta forma como podemos, dado un nuevo documento, revisar las palabras que aparecen en nuestro diccionario. Es evidente que debemos crear un diccionario con mejor preprocesamiento. Además, en el siguiente notebook se espera realizar un análisis tipo bag of words (en cuanto a la frecuencia de los tokens) pero a nivel de todos los tweets, con el fin de analizar la frecuencia de estos y poder obtener nuevas métricas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
